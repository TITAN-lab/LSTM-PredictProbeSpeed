{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "import pandas as pd\n",
    "import json, os\n",
    "from matplotlib import pyplot as plt\n",
    "import pywt\n",
    "from statsmodels.robust import mad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model (dir_path, model, history,cnt):\n",
    "    history_dict = history.history; \n",
    "    fname = dir_path + 'history_' + str(cnt) + '.json'\n",
    "    histfile = open(fname, 'w')\n",
    "    json.dump(history_dict, histfile)\n",
    "    histfile.close()\n",
    "    model_json = model.to_json()\n",
    "    mfname = dir_path + '/model_' + str(cnt) + '.json'\n",
    "    mname = dir_path + '/model_' + str(cnt) + '.h5'\n",
    "    with open(mfname, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(mname)\n",
    "    json_file.close()\n",
    "    \n",
    "\n",
    "    \n",
    "def LSTM_MODEL (epochs, neurons, dropout, batch_size, train_X,train_y, test_X, test_y ):\n",
    "    # default values: neurons=100;epochs=100;dropout=0.2;batch_size=200\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "    history = model.fit(train_X, train_y, epochs=epochs, batch_size=batch_size, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "    return [model, history]\n",
    "\n",
    "def norm_data_f(data, colv):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    data[colv] = min_max_scaler.fit_transform(data[colv].astype(float).values.reshape(-1,1))\n",
    "    return data\n",
    "\n",
    "def split_train_test(values):\n",
    "    n_train_hours = int(len(values)*(2/3))\n",
    "    train = values[:n_train_hours, :]\n",
    "    test = values[n_train_hours:, :]\n",
    "    # split into input and outputs\n",
    "    train_X, train_y = train[:, :-1], train[:, -1]\n",
    "    test_X, test_y = test[:, :-1], test[:, -1]\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "    return [train_X,train_y,test_X,test_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_path = 'datasets/training_datasets/Freeway12m_10months_1.csv'\n",
    "merged_data = pd.read_csv(training_data_path)\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clx = [ 'month','hour','dow','device_id','device_speed','meadianv','speed_2','INRIX_speed']\n",
    "cnt = -1\n",
    "neurons=100;epochs=[30,60,100];dropout=0.2;batch_size=200\n",
    "dir_path = 'results/epoch/'\n",
    "if not(os.path.isdir(dir_path)):\n",
    "    os.makedirs(dir_path)\n",
    "for epoch in epochs:\n",
    "    cnt+=1\n",
    "    new_col_data = merged_data[clx]\n",
    "    for norm_col in clx: \n",
    "        new_col_data = norm_data_f(new_col_data, norm_col)\n",
    "    values = new_col_data.values\n",
    "    [train_X,train_y,test_X,test_y] = split_train_test(values)\n",
    "    [model, history] = LSTM_MODEL (epoch, neurons, dropout, batch_size,train_X,train_y, test_X, test_y )\n",
    "    save_model (dir_path, model,history,epoch)\n",
    "    print('model ' + str(cnt) + ' completed ....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histFiles = os.listdir(dir_path)\n",
    "for histFile in histFiles:\n",
    "    if histFile.startswith('history'):\n",
    "        jname = dir_path + histFile.split('.')[0] + '.json'\n",
    "        print (jname)\n",
    "        json_file = open(jname, 'r')\n",
    "        loaded_model_json = json.load(json_file)\n",
    "        json_file.close()\n",
    "        output = loaded_model_json['val_mean_absolute_error']\n",
    "        output_main = loaded_model_json['mean_absolute_error']\n",
    "        plt.plot(output, label=histFile.split('_')[1])\n",
    "        plt.plot(output_main, label=histFile.split('_')[1])\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clx = [ 'month','hour','dow','device_id','device_speed','meadianv','speed_2','INRIX_speed']\n",
    "cnt = -1\n",
    "neurons=[50,100,250,500];epoch=100;dropout=0.2;batch_size=200\n",
    "dir_path = 'results/neuron/'\n",
    "if not(os.path.isdir(dir_path)):\n",
    "    os.makedirs(dir_path)\n",
    "for neuron in neurons:\n",
    "    cnt+=1\n",
    "    new_col_data = merged_data[clx]\n",
    "    for norm_col in clx: \n",
    "        new_col_data = norm_data_f(new_col_data, norm_col)\n",
    "    values = new_col_data.values\n",
    "    [train_X,train_y,test_X,test_y] = split_train_test(values)\n",
    "    [model, history] = LSTM_MODEL (epoch, neuron, dropout, batch_size,train_X,train_y, test_X, test_y )\n",
    "    save_model (dir_path, model,history,neuron)\n",
    "    print('model ' + str(cnt) + ' completed ....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histFiles = os.listdir(dir_path)\n",
    "for histFile in histFiles:\n",
    "    if histFile.startswith('history'):\n",
    "        jname = dir_path + histFile.split('.')[0] + '.json'\n",
    "        print (jname)\n",
    "        json_file = open(jname, 'r')\n",
    "        loaded_model_json = json.load(json_file)\n",
    "        json_file.close()\n",
    "        output = loaded_model_json['val_mean_absolute_error']\n",
    "        output_main = loaded_model_json['mean_absolute_error']\n",
    "        plt.plot(output, label=histFile.split('_')[1])\n",
    "        plt.plot(output_main, label=histFile.split('_')[1])\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clx = [ 'month','hour','dow','device_id','device_speed','meadianv','speed_2','INRIX_speed']\n",
    "cnt = -1\n",
    "neuron=100;epoch=100;dropout=0.2;batch_sizes=[50,100,200,500]\n",
    "dir_path = 'results/batchsize/'\n",
    "if not(os.path.isdir(dir_path)):\n",
    "    os.makedirs(dir_path)\n",
    "for batch_size in batch_sizes:\n",
    "    cnt+=1\n",
    "    new_col_data = merged_data[clx]\n",
    "    for norm_col in clx: \n",
    "        new_col_data = norm_data_f(new_col_data, norm_col)\n",
    "    values = new_col_data.values\n",
    "    [train_X,train_y,test_X,test_y] = split_train_test(values)\n",
    "    [model, history] = LSTM_MODEL (epoch, neuron, dropout, batch_size,train_X,train_y, test_X, test_y )\n",
    "    save_model (dir_path, model,history,batch_size)\n",
    "    print('model ' + str(cnt) + ' completed ....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histFiles = os.listdir(dir_path)\n",
    "for histFile in histFiles:\n",
    "    if histFile.startswith('history'):\n",
    "        jname = dir_path + histFile.split('.')[0] + '.json'\n",
    "        print (jname)\n",
    "        json_file = open(jname, 'r')\n",
    "        loaded_model_json = json.load(json_file)\n",
    "        json_file.close()\n",
    "        output = loaded_model_json['val_mean_absolute_error']\n",
    "        output_main = loaded_model_json['mean_absolute_error']\n",
    "        plt.plot(output, label=histFile.split('_')[1])\n",
    "        plt.plot(output_main, label=histFile.split('_')[1])\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_model (dir_path, model, history,batch_size,neuron, dropout, epoch, filename):\n",
    "    history_dict = history.history; \n",
    "    fname = dir_path + '_' + filename + '_'+'history_' + str(batch_size) + str(neuron) +str(dropout) +str(epoch) +'.json'\n",
    "    histfile = open(fname, 'w')\n",
    "    json.dump(history_dict, histfile)\n",
    "    histfile.close()\n",
    "    model_json = model.to_json()\n",
    "    mfname = dir_path + '_' + filename + '_'+'model_' + str(batch_size) + str(neuron) +str(dropout) +str(epoch) +'.json'\n",
    "    mname = dir_path + '_' + filename + '_'+ 'model_' + str(batch_size) + str(neuron) +str(dropout) +str(epoch) +'.h5'\n",
    "    with open(mfname, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(mname)\n",
    "    json_file.close()\n",
    "    \n",
    "def LSTM_MODEL_shuffle (epochs, neurons, dropout, batch_size, train_X,train_y, test_X, test_y ):\n",
    "    # default values: neurons=100;epochs=100;dropout=0.2;batch_size=200\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "    history = model.fit(train_X, train_y, epochs=epochs, batch_size=batch_size, validation_data=(test_X, test_y), verbose=2, shuffle=True)\n",
    "    return [model, history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clx = [ 'month','hour','dow','device_id','device_speed','meadianv','speed_2','INRIX_speed']\n",
    "cnt = -1\n",
    "# neurons=[10,50,100,200];epochs=[150];dropouts=[0.02,0.2,0.4];batch_sizes=[500]\n",
    "neurons=[200, 100, 10,50];epochs=[100, 30, 50];dropouts=[0.2];batch_sizes=[1000, 500, 200]\n",
    "dir_path = 'results/all/'\n",
    "training_data_folder = 'datasets/training_datasets/'\n",
    "all_csv_files = os.listdir(training_data_folder)\n",
    "for cur_tdata_file in all_csv_files:\n",
    "    training_data_path = training_data_folder + cur_tdata_file\n",
    "    filename = cur_tdata_file.split('.csv')[0]\n",
    "    merged_data = pd.read_csv(training_data_path)\n",
    "    if not(os.path.isdir(dir_path)):\n",
    "        os.makedirs(dir_path)\n",
    "    for batch_size in batch_sizes:\n",
    "        for neuron in neurons:\n",
    "            for dropout in dropouts:\n",
    "                for epoch in epochs:\n",
    "                    new_col_data = merged_data[clx]\n",
    "                    for norm_col in clx: \n",
    "                        new_col_data = norm_data_f(new_col_data, norm_col)\n",
    "                    values = new_col_data.values\n",
    "                [train_X,train_y,test_X,test_y] = split_train_test(values)\n",
    "                [model, history] = LSTM_MODEL_shuffle (epoch, neuron, dropout, batch_size,train_X,train_y, test_X, test_y )\n",
    "                save_all_model (dir_path, model,history,batch_size,neuron, dropout, epoch, filename)\n",
    "                print('model ' + str(cnt) + ' completed ....')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
