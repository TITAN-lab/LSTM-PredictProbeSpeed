{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import pywt\n",
    "from statsmodels.robust import mad\n",
    "import json, os\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_data_f(data, colv):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    data[colv] = min_max_scaler.fit_transform(data[colv].astype(float).values.reshape(-1,1))\n",
    "    return data\n",
    "\n",
    "def get_ori_data(raw_data_path):\n",
    "    cols = ['date','month','hour','minute','dow', 'season','device_id', 'device_speed','k_index', 'volume','device_distance', \n",
    "        'INRIX_speed']\n",
    "    data_df = pd.read_csv(raw_data_path)\n",
    "    data_df = data_df[cols]\n",
    "#     data_df = data_df[data_df['device_id'] == 11]\n",
    "    data_df_sorted = data_df.sort_values([\"device_id\",\"month\"])\n",
    "    dfx = data_df_sorted.drop_duplicates(subset=['month','hour', 'minute','device_id'])\n",
    "    hour_values = dfx[\"hour\"].unique(); device_values = dfx[\"device_id\"].unique();\n",
    "    month_values = dfx[\"month\"].unique();minute_values = dfx[\"minute\"].unique();\n",
    "    return [data_df_sorted, hour_values, minute_values, device_values, month_values]\n",
    "\n",
    "def get_ori_test_data(raw_data_path):\n",
    "    cols = ['date','month','hour','minute','dow', 'season','device_id', 'device_speed','k_index', 'volume','device_distance', \n",
    "        'INRIX_speed']\n",
    "    data_df = pd.read_csv(raw_data_path)\n",
    "    data_df = data_df[cols]\n",
    "#     data_df = data_df[data_df['device_id'] == 1]\n",
    "    data_df_sorted = data_df.sort_values([\"device_id\",\"month\"])\n",
    "    dfx = data_df_sorted.drop_duplicates(subset=['month','hour', 'minute','device_id'])\n",
    "    hour_values = dfx[\"hour\"].unique(); device_values = dfx[\"device_id\"].unique();\n",
    "    month_values = dfx[\"month\"].unique();minute_values = dfx[\"minute\"].unique();\n",
    "    return [data_df_sorted, hour_values, minute_values, device_values, month_values]\n",
    "\n",
    "def get_test_data(values):\n",
    "    n_train_hours = int(len(values)*(3/3))\n",
    "    test = values[:n_train_hours, :]\n",
    "    test_X, test_y = test[:, :-1], test[:, -1]\n",
    "    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "    return [test_X,test_y]\n",
    "\n",
    "def waveletSmooth( merged_data, smooth_columns):\n",
    "    db1 = pywt.Wavelet('db1')\n",
    "    for cur_col in smooth_columns:\n",
    "        x = merged_data[cur_col]\n",
    "        coeff = pywt.wavedec( x, db1)\n",
    "        sigma = mad( coeff[-5] )\n",
    "        coeff[1:] = ( pywt.threshold( i, value=sigma, mode=\"hard\" ) for i in coeff[1:] )\n",
    "        y = pywt.waverec( coeff, db1 )\n",
    "        if len(y) >merged_data.shape[0]:\n",
    "            y = y[0:-1]\n",
    "            merged_data[cur_col] = y\n",
    "        else:\n",
    "            merged_data[cur_col] = y\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(smooth, shift):\n",
    "    # example:\n",
    "    # raw_data_path = 'datasets/raw_data/Freeway12m_10months.csv'\n",
    "    # generate_training_data(raw_data_path)\n",
    "    train_folder = 'datasets/raw_data/train/'\n",
    "    all_files = os.listdir(train_folder)\n",
    "    for cur_file in all_files:\n",
    "        raw_train_data_path = train_folder + cur_file\n",
    "        filename = os.path.basename(cur_file).split('.csv')[0] + '_' + str(shift)\n",
    "        if os.path.isfile('datasets/training_datasets/' + filename + '.csv'):\n",
    "            os.remove('datasets/training_datasets/' + filename + '.csv')\n",
    "            print ('existing file removed!')\n",
    "        \n",
    "        out_cols = ['device_id','dow','month','hour', 'minute', 'meadianv', 'mean','stdv','min','max']\n",
    "\n",
    "        [data_df_sorted, hour_values, minute_values, device_values, month_values] = get_ori_data(raw_train_data_path)\n",
    "        done = []\n",
    "        outlist = []\n",
    "        output = pd.DataFrame(columns=out_cols)\n",
    "        output.to_csv('datasets/training_datasets/' + filename + '.csv', mode='a', header=True, index=False)\n",
    "        for device in device_values:\n",
    "            done.append(device)\n",
    "            print ('Training Data for ' + str(len(done)) + ' of ' + str(len(device_values)) + ' ids Generated')\n",
    "            for dow in range(0, 7):\n",
    "                for cur_month in month_values:\n",
    "                    for cur_hour in hour_values:\n",
    "                        for cur_min in minute_values:\n",
    "                            cur_data = data_df_sorted.loc[(data_df_sorted['device_id'] == device) &\n",
    "                                                          (data_df_sorted['month'] == cur_month) &\n",
    "                                                          (data_df_sorted['hour'] == cur_hour) & \n",
    "                                                          (data_df_sorted['minute'] == cur_min) &\n",
    "                                                          (data_df_sorted['dow']==dow)]\n",
    "                            if len(cur_data['INRIX_speed'].values)>0:\n",
    "                                minVal = cur_data['INRIX_speed'].values\n",
    "                                outlist = [int(device), int(dow), int(cur_month),\n",
    "                                           int(cur_hour), int(cur_min), int(np.median(minVal)), \n",
    "                                           int(np.mean(minVal)), float(np.std(minVal)), int(np.min(minVal)),int(np.max(minVal))]\n",
    "                                output.loc[len(output)] = outlist\n",
    "                    output.to_csv('datasets/training_datasets/' + filename + '.csv', mode='a', header=False, index=False)\n",
    "                    output = pd.DataFrame(columns=out_cols)\n",
    "    \n",
    "    \n",
    "        training_data = pd.read_csv('datasets/training_datasets/' + filename + '.csv'); \n",
    "        training_data['device_id'] = training_data['device_id'].astype(int)\n",
    "        training_data['dow'] = training_data['dow'].astype(int)\n",
    "        training_data['month'] = training_data['month'].astype(int)\n",
    "        training_data['hour'] = training_data['hour'].astype(int)\n",
    "        training_data['minute'] = training_data['minute'].astype(int)\n",
    "        merged_data = pd.merge(data_df_sorted, training_data, how='left', left_on=['device_id','dow','month','hour', 'minute'], \n",
    "                                      right_on=['device_id','dow','month','hour', 'minute'])\n",
    "        os.remove('datasets/training_datasets/' + filename + '.csv')\n",
    "        if smooth:\n",
    "            smooth_columns = ['INRIX_speed','device_speed', 'volume','meadianv' ]\n",
    "            merged_data = waveletSmooth(merged_data, smooth_columns)\n",
    "            merged_data['speed_2'] = merged_data['INRIX_speed'].shift(shift)\n",
    "            merged_data = merged_data[pd.notnull(merged_data['speed_2'])]\n",
    "            merged_data = merged_data[pd.notnull(merged_data['k_index'])]\n",
    "            merged_data.to_csv('datasets/training_datasets/' + filename + '.csv', index=False)\n",
    "        else:\n",
    "            merged_data['speed_2'] = merged_data['INRIX_speed'].shift(shift)\n",
    "            merged_data = merged_data[pd.notnull(merged_data['speed_2'])]\n",
    "            merged_data = merged_data[pd.notnull(merged_data['k_index'])]\n",
    "            merged_data.to_csv('datasets/training_datasets/' + filename + '.csv', index=False)\n",
    "        \n",
    "    \n",
    "#     print ('Training Data Completed .... ')\n",
    "#     file_path = 'datasets/training_datasets/' + filename + '.csv'\n",
    "#     return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_data(smooth, shift, new_cols):\n",
    "    test_folder = 'datasets/raw_data/test/'\n",
    "    all_files = os.listdir(test_folder)\n",
    "    for cur_file in all_files:\n",
    "        raw_test_data_path = test_folder + cur_file\n",
    "        filename = os.path.basename(cur_file).split('.csv')[0] + '_' + str(shift)\n",
    "        \n",
    "#     filename = os.path.basename(raw_test_data_path).split('.csv')[0]\n",
    "        if os.path.isfile('datasets/test_datasets/' + filename + '.csv'):\n",
    "            os.remove('datasets/test_datasets/' + filename + '.csv')\n",
    "            print ('existing file removed!')\n",
    "        test_out_cols = ['device_id','dow','month','hour', 'minute', 'meadianv', 'mean','stdv','min','max']\n",
    "        [test_data_df_sorted, hour_values, minute_values, device_values, month_values] = get_ori_test_data(raw_test_data_path)\n",
    "        done = []\n",
    "        outlist = []\n",
    "        output = pd.DataFrame(columns=test_out_cols)\n",
    "        output.to_csv('datasets/test_datasets/' + filename + '.csv', mode='a', header=True, index=False)\n",
    "        for device in device_values:\n",
    "            done.append(device)\n",
    "            print ('Test Data for ' + str(len(done)) + ' of ' + str(len(device_values)) + ' ids Generated ..')\n",
    "            for dow in range(0, 7):\n",
    "                for cur_month in month_values:\n",
    "                    for cur_hour in hour_values:\n",
    "                        for cur_min in minute_values:\n",
    "                            cur_data = test_data_df_sorted.loc[(test_data_df_sorted['device_id'] == device) &\n",
    "                                                          (test_data_df_sorted['month'] == cur_month) &\n",
    "                                                          (test_data_df_sorted['hour'] == cur_hour) & \n",
    "                                                          (test_data_df_sorted['minute'] == cur_min) &\n",
    "                                                          (test_data_df_sorted['dow']==dow)]\n",
    "                            if len(cur_data['INRIX_speed'].values)>0:\n",
    "                                minVal = cur_data['INRIX_speed'].values\n",
    "                                outlist = [int(device), int(dow), int(cur_month),\n",
    "                                           int(cur_hour), int(cur_min), int(np.median(minVal)), \n",
    "                                           int(np.mean(minVal)), int(np.std(minVal)), int(np.min(minVal)),int(np.max(minVal))]\n",
    "                                output.loc[len(output)] = outlist\n",
    "                    output.to_csv('datasets/test_datasets/' + filename + '.csv', mode='a', header=False, index=False)\n",
    "                    output = pd.DataFrame(columns=test_out_cols)\n",
    "\n",
    "        testing_data = pd.read_csv('datasets/test_datasets/' + filename + '.csv')\n",
    "        testing_data['device_id'] = testing_data['device_id'].astype(int)\n",
    "        testing_data['dow'] = testing_data['dow'].astype(int)\n",
    "        testing_data['month'] = testing_data['month'].astype(int)\n",
    "        testing_data['hour'] = testing_data['hour'].astype(int)\n",
    "        testing_data['minute'] = testing_data['minute'].astype(int)\n",
    "        merged_data = pd.merge(test_data_df_sorted, testing_data, how='left', left_on=['device_id','dow','month','hour', 'minute'], \n",
    "                                      right_on=['device_id','dow','month','hour', 'minute'])\n",
    "\n",
    "        os.remove('datasets/test_datasets/' + filename + '.csv')\n",
    "        if smooth:\n",
    "            print ('smoothing done')\n",
    "            smooth_columns = ['INRIX_speed','device_speed', 'volume','meadianv' ]\n",
    "            merged_data = waveletSmooth(merged_data, smooth_columns)\n",
    "            merged_data['speed_2'] = merged_data['INRIX_speed'].shift(shift)\n",
    "            merged_data = merged_data[pd.notnull(merged_data['speed_2'])]\n",
    "            merged_data = merged_data[pd.notnull(merged_data['k_index'])]\n",
    "            merged_data.to_csv('datasets/test_datasets/' + filename + '.csv', index=False)\n",
    "        else:\n",
    "            merged_data['speed_2'] = merged_data['INRIX_speed'].shift(shift)\n",
    "            merged_data = merged_data[pd.notnull(merged_data['speed_2'])]\n",
    "            merged_data = merged_data[pd.notnull(merged_data['k_index'])]\n",
    "            merged_data.to_csv('datasets/test_datasets/' + filename + '.csv', index=False)\n",
    "        \n",
    "#     file_path = 'datasets/test_datasets/' + filename + '.csv'\n",
    "#     print ('Generating Testing Parameters  Completed .... ')\n",
    "#     return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth=1; # for smoothing.. 0 for no smoothing\n",
    "shifts=[1,3,6,12]; # 15 minutes\n",
    "# shifts = [1]\n",
    "for shift in shifts:\n",
    "    generate_training_data(smooth, shift) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # raw_test_data_path = 'datasets/raw_data/Freeway12m_2months.csv'\n",
    "new_cols = [ 'month','hour','dow','device_id','device_speed','meadianv','speed_2','INRIX_speed']\n",
    "for shift in shifts:\n",
    "    generate_test_data(smooth, shift, new_cols) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
