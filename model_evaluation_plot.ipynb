{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = 'results/all/'\n",
    "histFiles = os.listdir(dir_path)\n",
    "# batch_size = str(500); epoch = str(150)\n",
    "# neurons=[10,50,100,200];epochs=[150];dropouts=[0.02,0.2,0.4];batch_sizes=[500]\n",
    "neurons=[200, 100, 10,50];epochs=[100, 30, 50];dropouts=[0.2];batch_sizes=[1000, 500, 200]\n",
    "training_data_folder = 'datasets/training_datasets/'\n",
    "all_csv_files = os.listdir(training_data_folder)\n",
    "for cur_tdata_file in all_csv_files:\n",
    "    filename = cur_tdata_file.split('.csv')[0]\n",
    "    for batch_size in batch_sizes:\n",
    "        for neuron in neurons:\n",
    "            for dropout in dropouts:\n",
    "                for epoch in epochs:\n",
    "                    fname = dir_path + '_' + filename + '_'+'history_' + str(batch_size) + str(neuron) +str(dropout) +str(epoch) +'.json'\n",
    "                    if os.path.isfile(fname):\n",
    "                        print (fname)\n",
    "                        json_file = open(fname, 'r')\n",
    "                        loaded_model_json = json.load(json_file)\n",
    "                        json_file.close()\n",
    "                        output = loaded_model_json['val_mean_absolute_error']\n",
    "                        output_main = loaded_model_json['mean_absolute_error']\n",
    "                        plt.plot(output)\n",
    "                        plt.plot(output_main)\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(values):\n",
    "    n_train_hours = int(len(values)*(3/3))\n",
    "    test = values[:n_train_hours, :]\n",
    "    test_X, test_y = test[:, :-1], test[:, -1]\n",
    "    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "    return [test_X,test_y]\n",
    "\n",
    "def test_data(test_data_path, new_cols, device_id):\n",
    "    merged_data = pd.read_csv(test_data_path)\n",
    "    merged_data = merged_data[new_cols]\n",
    "    merged_data = merged_data[merged_data['device_id'] == device_id]\n",
    "    for norm_col in new_cols: \n",
    "        merged_data = norm_data_f(merged_data, norm_col)\n",
    "    values_test = merged_data.values  \n",
    "    [x_testdata,y_testdata]=get_test_data(values_test)\n",
    "    return [x_testdata,y_testdata]\n",
    "\n",
    "def norm_data_f(data, colv):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    data[colv] = min_max_scaler.fit_transform(data[colv].astype(float).values.reshape(-1,1))\n",
    "    return data\n",
    "\n",
    "def plot_results(path, model, x_testdata, y_testdata):\n",
    "    p = percentage_difference(model, x_testdata, y_testdata)\n",
    "    data_read = pd.read_csv(path)\n",
    "    outx = denormalize(data_read, p)\n",
    "    newy_test = denormalize(data_read, y_testdata)\n",
    "    error = abs(sum(newy_test-outx)/len(outx))\n",
    "    return [outx, newy_test, error]\n",
    "    \n",
    "def percentage_difference(model, X_test, y_test):\n",
    "    percentage_diff=[]\n",
    "\n",
    "    p = model.predict(X_test)\n",
    "    for u in range(len(y_test)): # for each data index in test data\n",
    "        pr = p[u][0] # pr = prediction on day u\n",
    "\n",
    "        percentage_diff.append((pr-y_test[u]/pr)*100)\n",
    "    return p\n",
    "\n",
    "def denormalize(data_df, normalized_value):\n",
    "    df = data_df['INRIX_speed'].values.reshape(-1,1) \n",
    "    normalized_value = normalized_value.reshape(-1,1)\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    a = min_max_scaler.fit_transform(df)\n",
    "    new = min_max_scaler.inverse_transform(normalized_value)\n",
    "    return new   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = 'results/all/'\n",
    "raw_test_data_path = 'datasets/test_datasets/Freeway12m_2months_1.csv'\n",
    "filename = os.path.basename(raw_test_data_path).split('.csv')[0]\n",
    "cnt = -1; \n",
    "test_data_read = pd.read_csv(raw_test_data_path)\n",
    "device_ids = test_data_read['device_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clx = [ 'month','hour','dow','device_id','device_speed','meadianv','speed_2','INRIX_speed']\n",
    "# neurons=[10,50,100,200];epochs=[150];dropouts=[0.02,0.2,0.4];batch_sizes=[500]\n",
    "neurons=[200, 100, 10,50];epochs=[100, 30, 50];dropouts=[0.2];batch_sizes=[1000, 500, 200]\n",
    "cols =['filename',  'device_id', 'batchsize','neuron','mean', 'std', 'min', '25%', '50%', '75%', 'max'] \n",
    "output_pms =pd.DataFrame(columns=cols)\n",
    "\n",
    "testing_data_folder = 'datasets/test_datasets/'\n",
    "all_csv_files = os.listdir(testing_data_folder)\n",
    "output_pms.to_csv('results/error.csv', mode='a', header=True, index=False)\n",
    "for cur_tdata_file in all_csv_files:\n",
    "    filename = cur_tdata_file.split('.csv')[0]\n",
    "    filename = filename.replace('2months', '10months')\n",
    "    raw_test_data_path = testing_data_folder + cur_tdata_file\n",
    "    test_data_read = pd.read_csv(raw_test_data_path)\n",
    "    device_ids = test_data_read['device_id'].unique()\n",
    "    for device_id in device_ids:\n",
    "        [x_testdata,y_testdata] = test_data(raw_test_data_path, clx, device_id)\n",
    "        error_list = [];\n",
    "        for batch_size in batch_sizes:\n",
    "            for neuron in neurons:\n",
    "                for dropout in dropouts:\n",
    "                    for epoch in epochs:\n",
    "                        jname = dir_path + '_' + filename + '_'+ 'model_' + str(batch_size) + str(neuron) +str(dropout) +str(epoch) +'.json'\n",
    "                        fname = dir_path + '_' + filename + '_'+ 'model_' + str(batch_size) + str(neuron) +str(dropout) +str(epoch) +'.h5'\n",
    "                        if os.path.isfile(jname):\n",
    "                            print (jname + \"_\"+ str(device_id))                            \n",
    "                            json_file = open(jname, 'r')\n",
    "                            loaded_model_json = json_file.read()\n",
    "                            json_file.close()\n",
    "                            loaded_model = model_from_json(loaded_model_json)\n",
    "                            loaded_model.load_weights(fname)\n",
    "                            loaded_model.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "                            [outx, newy_test, error] = plot_results(raw_test_data_path, loaded_model, x_testdata, y_testdata)\n",
    "                            error_list.append(error)\n",
    "                            if len(error_list)>0:\n",
    "                                error_list_pms = [filename, device_id, batch_size, neuron, \n",
    "                                                  np.mean(error_list), np.std(error_list),\n",
    "                                                  np.min(error_list),np.percentile(error_list,25),\n",
    "                                                  np.percentile(error_list, 50),np.percentile(error_list, 75),\n",
    "                                                 np.max(error_list)] \n",
    "                                output_pms.loc[len(output_pms)] = error_list_pms\n",
    "            output_pms.to_csv('results/error.csv', mode='a', header=False, index=False)\n",
    "            output_pms =pd.DataFrame(columns=cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = output_pms.T\n",
    "tdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df = pd.read_csv('results/error.csv')\n",
    "error_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = error_df['filename'].unique()\n",
    "device_id_u = error_df['device_id'].unique()\n",
    "batch_u = error_df['batchsize'].unique() \n",
    "neuron_u = error_df['neuron'].unique()\n",
    "cols =['filename',  'batchsize_neuron','mean', 'std', 'min', '25%', '50%', '75%', 'max'] \n",
    "output_error_pms =pd.DataFrame(columns=cols)\n",
    "for filename in filenames:\n",
    "    for batch in batch_u:\n",
    "        for neuron in neuron_u:\n",
    "            filter_data = error_df[(error_df['filename']==filename) & (error_df['batchsize']==batch) \n",
    "                                   & (error_df['neuron']==neuron)]\n",
    "#             print (filter_data)\n",
    "            \n",
    "            cur_list = [filename, str(batch) + ':'+str(neuron), filter_data['mean'].mean(), \n",
    "                        filter_data['std'].mean(), filter_data['min'].mean(), filter_data['25%'].mean(),\n",
    "                        filter_data['50%'].mean(), filter_data['75%'].mean(), filter_data['max'].mean()]\n",
    "            output_error_pms.loc[len(output_error_pms)] = cur_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_error_pms.sort_values('mean').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_error_pms.to_csv('results/batch_neuron_error.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df = pd.read_csv('results/batch_neuron_error.csv')\n",
    "filenames = error_df['filename'].unique()\n",
    "labels = ['5-minute','60-minute','15-minute','30-minute']\n",
    "indx = 0\n",
    "for filename in filenames:\n",
    "    filter_data = error_df[(error_df['filename']==filename)]\n",
    "#     filter_data['mean'].plot()\n",
    "    plt.plot(filter_data['batchsize_neuron'], filter_data['mean'].values, label=labels[indx])\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.legend(loc=5)\n",
    "    \n",
    "    indx+=1\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df = pd.read_csv('results/batch_neuron_error.csv')\n",
    "filenames = error_df['filename'].unique()\n",
    "labels = ['5-minute','60-minute','15-minute','30-minute']\n",
    "indx = 0\n",
    "cols =['mean', 'std', 'min', '25%', '50%', '75%', 'max'] \n",
    "output_error_pmsx =pd.DataFrame(columns=cols)\n",
    "for filename in filenames:\n",
    "    filter_data = error_df[(error_df['filename']==filename)]\n",
    "    output_error_pmsx.loc[len(output_error_pmsx)] = filter_data[['mean', 'std', 'min', '25%', '50%', '75%', 'max']].mean().values\n",
    "#     filter_data[['mean', 'std', 'min', '25%', '50%', '75%', 'max']].mean().boxplot()\n",
    "#     plt.show()\n",
    "#     plt.plot(filter_data['batchsize_neuron'], filter_data['mean'].values, label=labels[indx])\n",
    "#     plt.xticks(rotation='vertical')\n",
    "#     plt.legend(loc=5)\n",
    "    \n",
    "#     indx+=1\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_error_pmsx.T.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_test(data_df, normalized_value):\n",
    "    df_attr =data_df[['hour', 'volume','device_distance']] \n",
    "    df = data_df['INRIX_speed'].values.reshape(-1,1) \n",
    "    normalized_value = normalized_value.reshape(-1,1)\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    a = min_max_scaler.fit_transform(df)\n",
    "    new = min_max_scaler.inverse_transform(normalized_value)\n",
    "#     print ('subset data')\n",
    "#     print (df_attr.shape)\n",
    "    return [new, df_attr]\n",
    "            \n",
    "def plot_results_test(path, device_id, model, x_testdata, y_testdata):\n",
    "    p = percentage_difference(model, x_testdata, y_testdata)\n",
    "    data_read = pd.read_csv(path)\n",
    "    data_read = data_read[data_read['device_id']==device_id]\n",
    "#     print ('original data')\n",
    "#     print (data_read.shape)\n",
    "    [outx,df_attr] = denormalize_test(data_read, p)\n",
    "    newy_test = denormalize(data_read, y_testdata)\n",
    "    error = abs(newy_test-outx)\n",
    "    df_attr['error'] = error\n",
    "#     print ('error')\n",
    "#     print (len(error))\n",
    "    return [outx, newy_test, error, df_attr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clx = [ 'month','hour','dow','device_id','device_speed','meadianv','speed_2','INRIX_speed']\n",
    "hyperPms = [[1000,100,0.2,50],[1000,50,0.2,50],[1000,10,0.2,50],[1000,200,0.2,50]]\n",
    "cols =['filename',  'device_id', 'hour','error'] \n",
    "output_pms_time =pd.DataFrame(columns=cols)\n",
    "\n",
    "testing_data_folder = 'datasets/test_datasets/'\n",
    "all_csv_files = os.listdir(testing_data_folder)\n",
    "output_pms_time.to_csv('results/error_time.csv', mode='a', header=True, index=False)\n",
    "cnt=0\n",
    "for cur_tdata_file in all_csv_files:\n",
    "    print (cur_tdata_file)\n",
    "    cur_hyperPms = hyperPms[cnt]\n",
    "    filename = cur_tdata_file.split('.csv')[0]\n",
    "    filename = filename.replace('2months', '10months')\n",
    "    jname = dir_path + '_' + filename + '_'+ 'model_' + str(cur_hyperPms[0]) + str(cur_hyperPms[1]) +str(cur_hyperPms[2]) +str(cur_hyperPms[3]) +'.json'\n",
    "    fname = dir_path + '_' + filename + '_'+ 'model_' + str(cur_hyperPms[0]) + str(cur_hyperPms[1]) +str(cur_hyperPms[2]) +str(cur_hyperPms[3]) +'.h5'\n",
    "    if os.path.isfile(jname):\n",
    "        print (jname)\n",
    "        raw_test_data_path = testing_data_folder + cur_tdata_file\n",
    "        test_data_read = pd.read_csv(raw_test_data_path)\n",
    "        device_ids = test_data_read['device_id'].unique()\n",
    "        json_file = open(jname, 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        loaded_model = model_from_json(loaded_model_json)\n",
    "        loaded_model.load_weights(fname)\n",
    "        loaded_model.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "        print ('model_loaded!!')\n",
    "        for device_id in device_ids:\n",
    "            [x_testdata,y_testdata] = test_data(raw_test_data_path, clx, device_id)\n",
    "            error_list = [];\n",
    "            [outx, newy_test, error, df_attr] = plot_results_test(raw_test_data_path,device_id, loaded_model, x_testdata, y_testdata)\n",
    "            unique_hour = df_attr['hour'].unique() \n",
    "            for cur_hour in unique_hour:\n",
    "                cur_hour_data = df_attr[df_attr['hour']==cur_hour]\n",
    "                error_hour = cur_hour_data['error'].mean()\n",
    "                output_pms_time.loc[len(output_pms_time)] = [cur_tdata_file, device_id, cur_hour, error_hour]\n",
    "            \n",
    "            output_pms_time.to_csv('results/error_time.csv', mode='a', header=False, index=False)\n",
    "            output_pms_time =pd.DataFrame(columns=cols)\n",
    "#             error_list.append(error)\n",
    "    \n",
    "    \n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_time = pd.read_csv('results/error_time.csv')\n",
    "labels = ['5-minute','60-minute','15-minute','30-minute']\n",
    "indx = 0\n",
    "for cur_file in error_time['filename'].unique():\n",
    "    \n",
    "    cur_data = error_time[error_time['filename']==cur_file]\n",
    "    hour_error_list = []\n",
    "    for cur_hour in cur_data['hour'].unique():\n",
    "        cur_data_hour = cur_data[cur_data['hour']==cur_hour]\n",
    "        hour_error_list.append(cur_data_hour['error'].mean())\n",
    "    plt.plot(hour_error_list, label=labels[indx])\n",
    "    plt.legend()\n",
    "    indx+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols =['0',  '1', '2','3','4',  '5', '6','7','8',  '9', '10','11','12',\n",
    "      '13','14','15','16','17','18','19','20','21','22','23'] \n",
    "heatmaps =pd.DataFrame(columns=cols)\n",
    "for cur_file in error_time['filename'].unique():\n",
    "    cur_data = error_time[error_time['filename']==cur_file]\n",
    "    for cur_device in cur_data['device_id'].unique():\n",
    "        cur_data_device = cur_data[cur_data['device_id']==cur_device]\n",
    "        cerror = cur_data_device.sort_values(['filename','device_id', 'hour'])\n",
    "        cerror = cerror['error'].values\n",
    "        heatmaps.loc[len(heatmaps)] = cerror\n",
    "    sns.heatmap(heatmaps, cmap='RdYlGn_r', vmin=0, vmax=12, annot=False)\n",
    "    plt.show()\n",
    "    heatmaps =pd.DataFrame(columns=cols)\n",
    "#         print (cur_data_device.sort_values(['filename','device_id', 'hour']).head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
