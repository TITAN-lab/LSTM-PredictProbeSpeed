{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "import pandas as pd\n",
    "import json, os\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_data_f(data, colv):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    data[colv] = min_max_scaler.fit_transform(data[colv].astype(float).values.reshape(-1,1))\n",
    "    return data\n",
    "\n",
    "def split_train_test(values):\n",
    "    n_train_hours = int(len(values)*(2/3))\n",
    "    train = values[:n_train_hours, :]\n",
    "    test = values[n_train_hours:, :]\n",
    "    # split into input and outputs\n",
    "    train_X, train_y = train[:, :-1], train[:, -1]\n",
    "    test_X, test_y = test[:, :-1], test[:, -1]\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "    return [train_X,train_y,test_X,test_y]\n",
    "\n",
    "def plot_acc(history, acc, val_acc):\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "    ax1 = axes[0]; ax2 = axes[1]\n",
    "    ax1.plot(history.history[acc], label='train')\n",
    "    ax1.plot(history.history[val_acc], label='test')\n",
    "    ax1.set_title('model accuracy')\n",
    "    ax1.set_ylabel(acc)\n",
    "    ax1.set_xlabel('epoch')\n",
    "    ax1.legend()\n",
    "    # summarize history for loss\n",
    "    ax2.plot(history.history['loss'])\n",
    "    ax2.plot(history.history['val_loss'])\n",
    "    ax2.set_title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    ax2.legend(['train', 'test'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def LSTM_MODEL (epochs, neurons, dropout, batch_size, train_X,train_y, test_X, test_y ):\n",
    "    # default values: neurons=100;epochs=100;dropout=0.2;batch_size=200\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "    history = model.fit(train_X, train_y, epochs=epochs, batch_size=batch_size, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "    return [model, history]\n",
    "\n",
    "def save_model (model, history,cnt):\n",
    "    history_dict = history.history; \n",
    "    fname = 'MODELS/history_' + str(cnt) + '.json'\n",
    "    histfile = open(fname, 'w')\n",
    "    json.dump(history_dict, histfile)\n",
    "    histfile.close()\n",
    "    model_json = model.to_json()\n",
    "    mfname = 'MODELS/model_' + str(cnt) + '.json'\n",
    "    mname = 'MODELS/model_' + str(cnt) + '.h5'\n",
    "    with open(mfname, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(mname)\n",
    "    json_file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_path = 'datasets/training_datasets/Freeway12m_10months_1.csv'\n",
    "merged_data = pd.read_csv(training_data_path)\n",
    "merged_data.head()\n",
    "col_types = [[ 'month','hour','dow','device_id','device_speed','meadianv','speed_2','INRIX_speed']] \n",
    "cnt = -1\n",
    "neurons=10;epochs=50;dropout=0.2;batch_size=1000\n",
    "for clx in col_types:\n",
    "    cnt+=1\n",
    "    new_col_data = merged_data[clx]\n",
    "    for norm_col in clx: \n",
    "        new_col_data = norm_data_f(new_col_data, norm_col)\n",
    "    values = new_col_data.values\n",
    "    [train_X,train_y,test_X,test_y] = split_train_test(values)    \n",
    "    [model, history] = LSTM_MODEL (epochs, neurons, dropout, batch_size,train_X,train_y, test_X, test_y )\n",
    "    save_model (model,history,cnt)\n",
    "    print('model ' + str(cnt) + ' completed ....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histFiles = os.listdir('MODELS')\n",
    "for histFile in histFiles:\n",
    "    if histFile.startswith('history'):\n",
    "        jname = \"MODELS/\" + histFile.split('.')[0] + '.json'\n",
    "        print (jname)\n",
    "        json_file = open(jname, 'r')\n",
    "        loaded_model_json = json.load(json_file)\n",
    "        json_file.close()\n",
    "        output = loaded_model_json['val_mean_absolute_error']\n",
    "        output_main = loaded_model_json['mean_absolute_error']\n",
    "        plt.plot(output, label=histFile.split('_')[1])\n",
    "        plt.plot(output_main, label=histFile.split('_')[1])\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_difference(model, X_test, y_test):\n",
    "    percentage_diff=[]\n",
    "\n",
    "    p = model.predict(X_test)\n",
    "    for u in range(len(y_test)): # for each data index in test data\n",
    "        pr = p[u][0] # pr = prediction on day u\n",
    "\n",
    "        percentage_diff.append((pr-y_test[u]/pr)*100)\n",
    "    return p\n",
    "\n",
    "def denormalize(data_df, normalized_value):\n",
    "    df = data_df['INRIX_speed'].values.reshape(-1,1) \n",
    "    normalized_value = normalized_value.reshape(-1,1)\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    a = min_max_scaler.fit_transform(df)\n",
    "    new = min_max_scaler.inverse_transform(normalized_value)\n",
    "    return new\n",
    "\n",
    "def plot_results(model, x_testdata, y_testdata, filename):\n",
    "    p = percentage_difference(model, x_testdata, y_testdata)\n",
    "    data_read = pd.read_csv('datasets/test_datasets/Freeway12m_2months_1.csv')\n",
    "    outx = denormalize(data_read, p)\n",
    "    newy_test = denormalize(data_read, y_testdata)\n",
    "    exp1=pd.DataFrame(outx,columns=['Prediction_speed'])\n",
    "    exp2=pd.DataFrame(newy_test, columns=['Actual_speed'])\n",
    "    exp=pd.concat([exp1, exp2], axis=1)\n",
    "    exp.to_csv('results/' + filename + '.csv')\n",
    "    print (abs(sum(newy_test-outx)/len(outx)))\n",
    "    plt.plot(newy_test,color='blue', label='Actual')\n",
    "    plt.plot(outx, color='red', label='Prediction')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(values):\n",
    "    n_train_hours = int(len(values)*(3/3))\n",
    "    test = values[:n_train_hours, :]\n",
    "    test_X, test_y = test[:, :-1], test[:, -1]\n",
    "    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "    return [test_X,test_y]\n",
    "\n",
    "def test_data(test_data_path, new_cols, device_id):\n",
    "    merged_data = pd.read_csv(test_data_path)\n",
    "    merged_data = merged_data[new_cols]\n",
    "    merged_data = merged_data[merged_data['device_id'] == device_id]\n",
    "    for norm_col in new_cols: \n",
    "        print (norm_col)\n",
    "        merged_data = norm_data_f(merged_data, norm_col)\n",
    "    values_test = merged_data.values  \n",
    "    [x_testdata,y_testdata]=get_test_data(values_test)\n",
    "    return [x_testdata,y_testdata] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test_data_path = 'datasets/test_datasets/Freeway12m_2months_1.csv'\n",
    "filename = os.path.basename(raw_test_data_path).split('.csv')[0]\n",
    "cnt = -1; device_ids = merged_data['device_id'].unique()\n",
    "for new_cols in col_types:\n",
    "    cnt+=1\n",
    "    for device_id in device_ids:\n",
    "        print (device_id)\n",
    "        [x_testdata,y_testdata] = test_data(raw_test_data_path, new_cols, device_id) \n",
    "        jname = 'MODELS/model_' + str(cnt) + '.json'\n",
    "        fname = 'MODELS/model_' + str(cnt) + '.h5'\n",
    "        print (jname)\n",
    "        json_file = open(jname, 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        loaded_model = model_from_json(loaded_model_json)\n",
    "        loaded_model.load_weights(fname)\n",
    "        loaded_model.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "        out_filename = filename+ '_'+str(device_id)\n",
    "        plot_results(loaded_model, x_testdata, y_testdata,out_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
